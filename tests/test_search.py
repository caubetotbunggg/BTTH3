import json
import os
import random
import sys
import traceback
from datetime import datetime

import chromadb
import numpy as np
from chromadb.config import Settings

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path ƒë·ªÉ import cleaner
sys.path.append(os.path.abspath("../BTTH3"))


def load_config():
    """Load configuration from JSON file."""
    try:
        with open("../BTTH3/index_config.json") as f:
            return json.load(f)
    except FileNotFoundError:
        print(" Kh√¥ng t√¨m th·∫•y file index_config.json")
        sys.exit(1)
    except json.JSONDecodeError:
        print(" File index_config.json kh√¥ng h·ª£p l·ªá")
        sys.exit(1)


def initialize_client(config):
    """Initialize ChromaDB client and collection."""
    try:
        client = chromadb.PersistentClient(path=config["persist_directory"])

        # Ki·ªÉm tra collection c√≥ t·ªìn t·∫°i kh√¥ng
        try:
            collection = client.get_collection(config["collection_name"])
            print(f" K·∫øt n·ªëi th√†nh c√¥ng v·ªõi collection: {config['collection_name']}")
            return client, collection
        except Exception as e:
            print(
                f" Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi collection '{config['collection_name']}': {e}"
            )
            print(" H√£y ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ ch·∫°y script indexing tr∆∞·ªõc ƒë√≥")
            sys.exit(1)

    except Exception as e:
        print(f" L·ªói khi kh·ªüi t·∫°o ChromaDB client: {e}")
        sys.exit(1)


def load_sample_data(embedding_dir, n_samples=10):
    """Load sample data from embedding files."""
    if not os.path.exists(embedding_dir):
        print(f" Th∆∞ m·ª•c embedding kh√¥ng t·ªìn t·∫°i: {embedding_dir}")
        sys.exit(1)

    files = [f for f in os.listdir(embedding_dir) if f.endswith(".npy")]
    if not files:
        print(f" Kh√¥ng t√¨m th·∫•y file embedding n√†o trong: {embedding_dir}")
        sys.exit(1)

    print(f"üìÇ T√¨m th·∫•y {len(files)} file embedding")

    samples = []
    selected_files = random.sample(files, min(n_samples, len(files)))

    for file in selected_files:
        file_id = file.replace(".npy", "")
        path = os.path.join(embedding_dir, file)

        try:
            vectors = np.load(path)

            # X·ª≠ l√Ω shape c·ªßa vectors
            if len(vectors.shape) == 1:
                vectors = [vectors.tolist()]
            else:
                vectors = vectors.tolist()

            # L·∫•y random m·ªôt vector t·ª´ file n√†y
            if vectors:
                random_idx = random.randint(0, len(vectors) - 1)
                chunk_id = f"{file_id}_{random_idx}"
                samples.append((chunk_id, vectors[random_idx]))

                if len(samples) >= n_samples:
                    break

        except Exception as e:
            print(f" L·ªói khi load file {file}: {e}")
            continue

    print(f" ƒê√£ chu·∫©n b·ªã {len(samples)} sample queries")
    return samples


def perform_search(collection, samples, k=5):
    """Perform similarity search for all samples."""
    results = []

    print(f"üîç B·∫Øt ƒë·∫ßu truy v·∫•n {len(samples)} samples...")

    for i, (chunk_id, vec) in enumerate(samples, 1):
        try:
            print(f"  [{i}/{len(samples)}] Truy v·∫•n: {chunk_id}")

            query_result = collection.query(
                query_embeddings=[vec],
                n_results=k,
                include=["distances", "metadatas", "documents"],
            )

            # X·ª≠ l√Ω k·∫øt qu·∫£
            ids = query_result["ids"][0]
            distances = query_result["distances"][0]
            metadatas = query_result["metadatas"][0]
            documents = query_result["documents"][0]

            search_results = []
            for rank, (res_id, score, meta, doc) in enumerate(
                zip(ids, distances, metadatas, documents)
            ):
                similarity = 1 - score
                law_id = meta.get("law_id", "N/A")
                title = meta.get("title", "N/A")
                date = meta.get("date", "N/A")
                chunk_num = res_id.split("_")[-1]

                search_results.append(
                    {
                        "rank": rank + 1,
                        "id": res_id,
                        "law_id": law_id,
                        "title": title,
                        "date": date,
                        "chunk_num": chunk_num,
                        "similarity": similarity,
                        "distance": score,
                        "document": (
                            doc[:200] + "..." if len(doc) > 200 else doc
                        ),  # Truncate long documents
                    }
                )

            results.append({"query_id": chunk_id, "results": search_results})

        except Exception as e:
            print(f" L·ªói khi truy v·∫•n {chunk_id}: {e}")
            continue

    print(f" Ho√†n th√†nh truy v·∫•n {len(results)} samples th√†nh c√¥ng")
    return results


def generate_markdown_report(results, output_path):
    """Generate markdown report from search results."""
    results_md = [
        "#  K·∫øt qu·∫£ Truy v·∫•n T∆∞∆°ng t·ª± (Top-5)\n",
        f"**Th·ªùi gian t·∫°o:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        f"**T·ªïng s·ªë truy v·∫•n:** {len(results)}\n",
        "---\n",
    ]

    for i, result in enumerate(results, 1):
        query_id = result["query_id"]
        search_results = result["results"]

        results_md.append(f"## üîπ Query {i}: `{query_id}`\n")

        if not search_results:
            results_md.append(" Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t√¨m th·∫•y\n")
            continue

        # Th√™m b·∫£ng k·∫øt qu·∫£
        results_md.append(
            "| Rank | ID | Lu·∫≠t | Ti√™u ƒë·ªÅ | Ng√†y | Chunk | Similarity | Distance |"
        )
        results_md.append(
            "|------|----|----- |---------|------|-------|------------|----------|"
        )

        for res in search_results:
            results_md.append(
                f"| {res['rank']} | `{res['id']}` | `{res['law_id']}` | **{res['title'][:50]}{'...' if len(res['title']) > 50 else ''}** | {res['date']} | {res['chunk_num']} | {res['similarity']:.4f} | {res['distance']:.4f} |"
            )

        results_md.append("")

        # Th√™m n·ªôi dung document ƒë·∫ßu ti√™n
        if search_results:
            best_match = search_results[0]
            results_md.append(f"**N·ªôi dung t∆∞∆°ng t·ª± nh·∫•t:**")
            results_md.append(f"```")
            results_md.append(best_match["document"])
            results_md.append(f"```")
            results_md.append("")

        results_md.append("---\n")

    # Th√™m th·ªëng k√™ t·ªïng quan
    results_md.append("## Th·ªëng k√™ T·ªïng quan\n")

    # Th·ªëng k√™ similarity scores
    all_similarities = []
    for result in results:
        for res in result["results"]:
            all_similarities.append(res["similarity"])

    if all_similarities:
        avg_similarity = np.mean(all_similarities)
        max_similarity = np.max(all_similarities)
        min_similarity = np.min(all_similarities)

        results_md.append(f"- **Similarity trung b√¨nh:** {avg_similarity:.4f}")
        results_md.append(f"- **Similarity cao nh·∫•t:** {max_similarity:.4f}")
        results_md.append(f"- **Similarity th·∫•p nh·∫•t:** {min_similarity:.4f}")
        results_md.append("")

    # Th·ªëng k√™ theo lu·∫≠t
    law_counts = {}
    for result in results:
        for res in result["results"]:
            law_id = res["law_id"]
            law_counts[law_id] = law_counts.get(law_id, 0) + 1

    if law_counts:
        results_md.append("### üìö Ph√¢n b·ªë theo Lu·∫≠t:")
        sorted_laws = sorted(law_counts.items(), key=lambda x: x[1], reverse=True)
        for law_id, count in sorted_laws[:10]:  # Top 10
            results_md.append(f"- **{law_id}:** {count} k·∫øt qu·∫£")

    # L∆∞u file
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(results_md))

    print(f" ƒê√£ l∆∞u b√°o c√°o t·∫°i: {output_path}")


def main():
    """Main function to run the search test."""
    print(" B·∫Øt ƒë·∫ßu ki·ªÉm tra t√¨m ki·∫øm t∆∞∆°ng t·ª±...")

    # C·∫•u h√¨nh
    config = load_config()
    embedding_dir = "../BTTH3/data/processed/embeddings"
    k = 5  # top-k
    n_samples = 10  # s·ªë sample query

    # Kh·ªüi t·∫°o client v√† collection
    client, collection = initialize_client(config)

    # Ki·ªÉm tra s·ªë l∆∞·ª£ng documents trong collection
    try:
        total_docs = collection.count()
        print(f" Collection hi·ªán c√≥ {total_docs:,} documents")

        if total_docs == 0:
            print(" Collection tr·ªëng! H√£y ch·∫°y script indexing tr∆∞·ªõc")
            sys.exit(1)

    except Exception as e:
        print(f" Kh√¥ng th·ªÉ ƒë·∫øm documents: {e}")

    # Load sample data
    samples = load_sample_data(embedding_dir, n_samples)

    if not samples:
        print(" Kh√¥ng th·ªÉ t·∫°o sample data")
        sys.exit(1)

    # Perform search
    results = perform_search(collection, samples, k)

    if not results:
        print(" Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o")
        sys.exit(1)

    # Generate report
    output_path = "../BTTH3/docs/search_results.md"
    generate_markdown_report(results, output_path)

    print(" Ho√†n th√†nh ki·ªÉm tra t√¨m ki·∫øm!")
    print(f" ƒê√£ test {len(results)} queries v·ªõi {k} k·∫øt qu·∫£ m·ªói query")
    print(f" Xem b√°o c√°o chi ti·∫øt t·∫°i: {output_path}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        print(f"\n L·ªói kh√¥ng mong mu·ªën: {e}")
        traceback.print_exc()
